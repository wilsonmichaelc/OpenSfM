#!/usr/bin/env python

import argparse
import datetime
import logging
import os.path
import shutil
import yaml
import math

from itertools import combinations
from timeit import default_timer as timer

from opensfm import bow
from opensfm import commands
from opensfm import dataset
from opensfm import features
from opensfm import io
from opensfm import log
from opensfm import matching

from opensfm import csfm


logger = logging.getLogger(__name__)
log.setup()


def create_bench_dir(dataset):
    base_dir = os.path.dirname(os.path.normpath(dataset)) if \
        os.path.isdir(dataset) else \
        os.path.dirname(dataset)

    dataset_dir = os.path.basename(os.path.normpath(dataset))

    bench_dir = os.path.join(
        base_dir,
        dataset_dir + \
        "__bench_feat_match__" + \
        datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

    io.mkdir_p(bench_dir)

    return bench_dir


def create_dataset(orig_dir, bench_dir, config):
    if config["matcher_type"] == "FLANN":
        return create_flann_dataset(orig_dir, bench_dir, config)
    elif config["matcher_type"] == "WORDS":
        return create_words_dataset(orig_dir, bench_dir, config)
    elif config["matcher_type"] == "CASCADE_HASHING":
        return create_cascade_hashing_dataset(orig_dir, bench_dir, config)
    elif config["matcher_type"] == "MATRIX":
        return create_matrix_dataset(orig_dir, bench_dir, config)
    else:
        raise ValueError("Invalid matcher_type: {}".format(
            matcher_type))


def overwrite_config(config, work_dir):
    orig_config = load_config(work_dir)

    for key, value in config.items():
        orig_config[key] = value

    save_config(orig_config, work_dir)


def create_brute_force_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "bruteforce")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def create_flann_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "flann")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        p, f, c = data.load_features(image)
        index = features.build_flann_index(f, data.config)
        data.save_feature_index(image, index)

    return data


def create_words_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "words")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        bows = bow.load_bows(data.config)
        n_closest = data.config['bow_words_to_match']
        p, f, c = data.load_features(image)
        closest_words = bows.map_to_words(
            f, n_closest, data.config['bow_matcher_type'])
        data.save_words(image, closest_words)

    return data


def create_matrix_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "matrix")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def create_cascade_hashing_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "cascade_hashing")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def load_config(work_dir):
    config = {}
    filepath = os.path.join(work_dir, "config.yaml")
    if os.path.isfile(filepath):
        with open(filepath) as fin:
            new_config = yaml.safe_load(fin)
        if new_config:
            for k, v in new_config.items():
                config[k] = v

    return config


def save_config(config, work_dir):
    with open(os.path.join(work_dir, 'config.yaml'), 'w') as fout:
        yaml.dump(config, fout, default_flow_style=False)


def match(data):
    images = data.images()
    images.sort()

    pairs = combinations(images, 2)
    comb = {im: [] for im in images}
    for im1, im2 in pairs:
        comb[im1].append(im2)

    matcher_type = data.config['matcher_type']

    matching_time = []

    for im1 in comb:
        im1_matches = {}

        for im2 in comb[im1]:
            # symmetric matching
            p1, f1, c1 = data.load_features(im1)
            p2, f2, c2 = data.load_features(im2)

            if matcher_type == 'WORDS':
                w1 = data.load_words(im1)
                w2 = data.load_words(im2)
                t = timer()
                matches = matching.match_words_symmetric(
                    f1, w1, f2, w2,data.config)
            elif matcher_type == 'FLANN':
                i1 = data.load_feature_index(im1, f1)
                i2 = data.load_feature_index(im2, f2)
                t = timer()
                matches = matching.match_flann_symmetric(
                    f1, i1, f2, i2,data.config)
            elif matcher_type == 'MATRIX':
                t = timer()
                matches = matching.match_matrix_symmetric(f1, f2, data.config)
            elif matcher_type == 'BRUTEFORCE':
                t = timer()
                matches = matching.match_brute_force_symmetric(
                    f1, f2, data.config)
            elif matcher_type == "CASCADE_HASHING":
                t = timer()
                matches = csfm.match_using_cascade_hashing(
                    f1, f2, data.config['lowes_ratio'])
            else:
                raise ValueError("Invalid matcher_type: {}".format(
                    matcher_type))

            matching_time.append(timer() - t)

            im1_matches[im2] = matches

            logger.debug('{} - {} has {} initial matches'.format(
                im1, im2, len(matches)))

            logger.info('Matching time ({0}, {1}-{2}, {3}-{4}): {5}s'.format(
                matcher_type, im1, im2, len(f1), len(f2), matching_time[-1]))

        data.save_matches(im1, im1_matches)

    mean_time = sum(matching_time) / len(matching_time)

    logger.debug("Average matching time ({}): {}".format(
        matcher_type,
        mean_time))

    return mean_time


def load_matches(data):
    matches = {}
    for im1 in data.images():
        m = data.load_matches(im1)
        for im2 in m:
            matches[tuple(sorted([im1, im2]))] = m[im2]

    return matches


def benchmark_matches(matches, truth):
    res = {
        "true_pos": [],
        "false_pos": [],
        "false_neg": [],
    }

    for pair in truth:
        if pair not in matches:
            res["false_neg"] = truth[pair]
            continue

        m = matches[pair]
        for match in truth[pair]:
            if match in m:
                res["true_pos"].append(match)
            else:
                res["false_neg"].append(match)

    for pair in matches:
        if pair not in truth:
            res["false_pos"] = matches[pair]
            continue

        t = truth[pair]
        for match in matches[pair]:
            if match not in t:
                res["false_pos"].append(match)

    return res


def match_and_report(data, bench_matches):
    matching_time = match(data)
    matches = load_matches(data)
    bench_res = benchmark_matches(matches, bench_matches)
    report(bench_res, data.config["matcher_type"])

    precision, recall = precision_recall(bench_res)

    return matching_time, precision, recall


def precision_recall(bench_res):
    true_pos = len(bench_res["true_pos"])
    false_pos = len(bench_res["false_pos"])
    false_neg = len(bench_res["false_neg"])

    precision = 0 if true_pos == 0 else float(true_pos) / (true_pos + false_pos)
    recall = 0 if true_pos == 0 else float(true_pos) / (true_pos + false_neg)

    return precision, recall


def report(res, method_name):
    p, r = precision_recall(res)

    logger.info("=============================")
    logger.info("Report for {}".format(method_name))
    logger.info("Precision: {}".format(p))
    logger.info("Recall: {}".format(r))
    logger.info("=============================")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Benchmark feature matching algorithms')
    parser.add_argument('--dataset', help='path to the dataset to be processed')

    args = parser.parse_args()

    datasets = [args.dataset] if args.dataset else \
        [
            "data/benchmark_matching/amsterdam",
            "data/benchmark_matching/copenhagen",
            "data/benchmark_matching/lemans",
            "data/benchmark_matching/malmo",
            "data/benchmark_matching/malmohus",
            "data/benchmark_matching/molleberga",
            "data/benchmark_matching/portland",
            "data/benchmark_matching/stapeln",
        ]

    lowes_ratio = 1.0

    b_matches = {}
    for d in datasets:
        args.dataset = d
        overwrite_config({ }, d)

        commands.extract_metadata.Command().run(args)
        commands.detect_features.Command().run(args)

        bench_dir = create_bench_dir(d)
        b_data = create_brute_force_dataset(d, bench_dir, { "lowes_ratio": lowes_ratio, "matcher_type": "BRUTEFORCE" })
        match(b_data)
        b_matches[d] = load_matches(b_data)

    run_configs = [
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 100, "flann_branching": 16, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 100, "flann_branching": 16, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 100, "flann_branching": 16, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 4, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 4, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 4, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 4, "flann_checks": 5 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 8, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 8, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 8, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 16, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 16, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 16, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 64, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 64, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 10, "flann_branching": 64, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 4, "flann_branching": 16, "flann_checks": 200 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 4, "flann_branching": 16, "flann_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "FLANN", "flann_iterations": 4, "flann_branching": 16, "flann_checks": 10 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "WORDS", "bow_num_checks": 100 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "WORDS", "bow_num_checks": 20 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "WORDS", "bow_num_checks": 5 },
        { "lowes_ratio": lowes_ratio, "matcher_type": "CASCADE_HASHING" },
    ]

    bench_results =  []
    for c in run_configs:
        bench_result = {
            "config": c,
            "datasets": [],
            "matching_times": [],
            "precisions": [],
            "recalls": []
        }

        for d in datasets:
            bench_dir = create_bench_dir(d)
            t_data = create_dataset(d, bench_dir, c)
            t, p, r = match_and_report(t_data, b_matches[d])

            bench_result["datasets"].append(d)
            bench_result["matching_times"].append(str(t))
            bench_result["precisions"].append(str(p))
            bench_result["recalls"].append(str(r))

        bench_results.append(bench_result)

    for br in bench_results:
        logger.info("========================================")
        run_definition = "Report - "
        for key, value in br["config"].items():
            run_definition += key + ": " + str(value) + ", "

        logger.info(run_definition)
        logger.info("Dataset:")
        logger.info("\t".join(br["datasets"]))
        logger.info("Matching time:")
        logger.info("\t".join(br["matching_times"]))
        logger.info("Precision:")
        logger.info("\t".join(br["precisions"]))
        logger.info("Recall:")
        logger.info("\t".join(br["recalls"]))
        logger.info("========================================")
